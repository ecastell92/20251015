"""
Lambda function: filter_inventory - CORREGIDO
--------------------------------
Filtra el inventario de S3 para generar un manifiesto con los objetos que deben
respaldarse, seg√∫n criticidad y checkpoint previo.

CAMBIOS:
- Eliminado c√≥digo duplicado (utils.py integrado aqu√≠)
- Cifrado unificado a AES256 (sin KMS)
- MANIFEST_BUCKET es el mismo que BACKUP_BUCKET
"""

import boto3
import os
import json
import gzip
import csv
import io
import uuid
import logging
from datetime import datetime, timezone
from typing import Dict, Optional

logger = logging.getLogger()
logger.setLevel(os.environ.get("LOG_LEVEL", "INFO").upper())

s3_client = boto3.client("s3")

# Configuraci√≥n - Un solo bucket central
BACKUP_BUCKET = os.environ.get("BACKUP_BUCKET")  # Bucket central √∫nico
FORCE_FULL_ON_FIRST_RUN = os.environ.get("FORCE_FULL_ON_FIRST_RUN", "false").lower() == "true"

try:
    FALLBACK_MAX_OBJECTS = int(os.environ.get("FALLBACK_MAX_OBJECTS", "0") or 0)
except ValueError:
    FALLBACK_MAX_OBJECTS = 0

try:
    FALLBACK_TIME_LIMIT_SECONDS = int(os.environ.get("FALLBACK_TIME_LIMIT_SECONDS", "0") or 0)
except ValueError:
    FALLBACK_TIME_LIMIT_SECONDS = 0

# Prefijos permitidos por criticidad
try:
    ALLOWED_PREFIXES = json.loads(os.environ.get("ALLOWED_PREFIXES", "{}"))
except json.JSONDecodeError:
    ALLOWED_PREFIXES = {}

# Tama√±o m√≠nimo de parte para multipart upload (6 MiB)
MIN_CHUNK_SIZE_BYTES = 6 * 1024 * 1024

# ============================================================================
# CHECKPOINT MANAGEMENT - Implementaci√≥n √∫nica
# ============================================================================

def read_checkpoint(bucket: str, source_bucket: str, backup_type: str) -> Optional[datetime]:
    """Lee la marca de tiempo del √∫ltimo checkpoint desde un archivo en S3."""
    checkpoint_key = f"checkpoints/{source_bucket}/{backup_type}.txt"
    try:
        logger.info(f"üîé Leyendo checkpoint desde s3://{bucket}/{checkpoint_key}")
        response = s3_client.get_object(Bucket=bucket, Key=checkpoint_key)
        timestamp_str = response["Body"].read().decode("utf-8")
        checkpoint = datetime.fromisoformat(timestamp_str)
        logger.info(f"‚úÖ Checkpoint encontrado: {checkpoint.isoformat()}")
        return checkpoint
    except s3_client.exceptions.NoSuchKey:
        logger.warning(f"‚ö†Ô∏è No se encontr√≥ checkpoint. Primera ejecuci√≥n o backup completo.")
        return None
    except Exception as e:
        logger.error(f"‚ùå Error al leer checkpoint: {e}", exc_info=True)
        return None


def write_checkpoint(bucket: str, source_bucket: str, backup_type: str, timestamp: datetime):
    """Escribe la marca de tiempo del checkpoint actual en un archivo en S3."""
    checkpoint_key = f"checkpoints/{source_bucket}/{backup_type}.txt"
    timestamp_str = timestamp.isoformat()
    try:
        logger.info(f"üíæ Escribiendo checkpoint: s3://{bucket}/{checkpoint_key} = {timestamp_str}")
        s3_client.put_object(
            Bucket=bucket,
            Key=checkpoint_key,
            Body=timestamp_str.encode("utf-8"),
            ServerSideEncryption="AES256"  # ‚úÖ Cifrado unificado
        )
        logger.info("‚úÖ Checkpoint guardado correctamente.")
    except Exception as e:
        logger.error(f"‚ùå Error al escribir checkpoint: {e}", exc_info=True)


# ============================================================================
# INVENTORY MANIFEST DISCOVERY
# ============================================================================

def find_latest_inventory_manifest(bucket: str, prefix: str) -> Optional[str]:
    """Busca y devuelve la clave del manifest.json m√°s reciente dentro del prefijo."""
    logger.info(f"üîç Buscando manifiesto de inventario en s3://{bucket}/{prefix}")
    if not prefix.endswith("/"):
        prefix += "/"

    paginator = s3_client.get_paginator("list_objects_v2")
    latest_obj = None

    try:
        for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
            for obj in page.get("Contents", []):
                if obj["Key"].endswith("manifest.json"):
                    if latest_obj is None or obj["LastModified"] > latest_obj["LastModified"]:
                        latest_obj = obj
        
        if latest_obj:
            logger.info(f"‚úÖ Manifiesto encontrado: {latest_obj['Key']}")
            return latest_obj["Key"]
        else:
            logger.warning(f"‚ö†Ô∏è No se encontr√≥ manifest.json en {prefix}")
            return None
    except Exception as e:
        logger.error(f"‚ùå Error al buscar manifiestos: {e}", exc_info=True)
        return None


# ============================================================================
# MANIFEST GENERATION FROM INVENTORY
# ============================================================================

def get_column_indices(schema_string: str) -> Dict[str, int]:
    """Obtiene los √≠ndices de las columnas necesarias del schema del inventario."""
    columns = [c.strip() for c in schema_string.split(",")]
    required = ["Bucket", "Key", "LastModifiedDate"]
    for field in required:
        if field not in columns:
            raise ValueError(f"Inventory schema missing required field: {field}")
    return {
        "bucket": columns.index("Bucket"),
        "key": columns.index("Key"),
        "last_modified": columns.index("LastModifiedDate"),
    }


def stream_inventory_to_manifest(
    backup_bucket: str,
    manifest: Dict,
    source_bucket: str,
    backup_type: str,
    criticality: str,
    last_checkpoint: Optional[datetime],
) -> Optional[Dict[str, str]]:
    """Lee los data files del inventario y genera un manifiesto CSV con los objetos cambiados."""
    
    temp_manifest_key = f"manifests/temp/{source_bucket}-{uuid.uuid4()}.csv"

    logger.info(f"üìù Generando manifiesto temporal en s3://{backup_bucket}/{temp_manifest_key}")

    # Crear multipart upload con AES256
    mpu = s3_client.create_multipart_upload(
        Bucket=backup_bucket,
        Key=temp_manifest_key,
        ServerSideEncryption='AES256'  # ‚úÖ Cifrado unificado
    )

    upload_id = mpu["UploadId"]
    parts = []
    part_number = 1
    buffer = io.StringIO()
    writer = csv.writer(buffer)
    objects_found = 0

    prefixes = ALLOWED_PREFIXES.get(criticality, [])
    indices = get_column_indices(manifest["fileSchema"])
    idx_bucket = indices["bucket"]
    idx_key = indices["key"]
    idx_last_modified = indices["last_modified"]

    try:
        for file_info in manifest.get("files", []):
            data_key = file_info.get("key")
            if not data_key:
                logger.warning("‚ö†Ô∏è Entrada de archivo vac√≠a en manifiesto, saltando...")
                continue

            # Limpiar rutas duplicadas
            duplicate_pattern = f"/{source_bucket}//{source_bucket}/"
            if duplicate_pattern in data_key:
                correct_segment = f"/{source_bucket}/"
                data_key = data_key.replace(duplicate_pattern, correct_segment)
            data_key = data_key.replace('//', '/')

            logger.info(f"üìñ Leyendo data file: s3://{backup_bucket}/{data_key}")

            try:
                obj = s3_client.get_object(Bucket=backup_bucket, Key=data_key)
            except s3_client.exceptions.NoSuchKey:
                logger.warning(f"‚ö†Ô∏è Archivo no encontrado: s3://{backup_bucket}/{data_key}")
                continue

            with gzip.GzipFile(fileobj=obj["Body"]) as gz:
                reader = csv.reader(io.TextIOWrapper(gz))
                for row in reader:
                    object_bucket = row[idx_bucket]
                    object_key = row[idx_key]
                    last_modified_str = row[idx_last_modified]

                    try:
                        last_mod_dt = datetime.fromisoformat(last_modified_str.replace("Z", "+00:00"))
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Error procesando fecha '{last_modified_str}': {e}")
                        continue

                    # Filtrar por prefijos permitidos
                    if prefixes and not any(object_key.startswith(p) for p in prefixes):
                        continue
                    
                    # Filtrar por checkpoint (solo para incrementales)
                    if last_checkpoint and last_mod_dt <= last_checkpoint:
                        continue

                    writer.writerow([object_bucket, object_key])
                    objects_found += 1

                    # Subir parte cuando se alcanza el tama√±o m√≠nimo
                    if buffer.tell() >= MIN_CHUNK_SIZE_BYTES:
                        part = s3_client.upload_part(
                            Bucket=backup_bucket,
                            Key=temp_manifest_key,
                            PartNumber=part_number,
                            UploadId=upload_id,
                            Body=buffer.getvalue().encode("utf-8"),
                        )
                        parts.append({"PartNumber": part_number, "ETag": part["ETag"]})
                        part_number += 1
                        buffer = io.StringIO()
                        writer = csv.writer(buffer)

        # Subir la √∫ltima parte si hay datos
        if buffer.tell() > 0:
            part = s3_client.upload_part(
                Bucket=backup_bucket,
                Key=temp_manifest_key,
                PartNumber=part_number,
                UploadId=upload_id,
                Body=buffer.getvalue().encode("utf-8"),
            )
            parts.append({"PartNumber": part_number, "ETag": part["ETag"]})

        if objects_found == 0:
            logger.info("‚ÑπÔ∏è No se encontraron objetos nuevos. Abortando subida.")
            s3_client.abort_multipart_upload(
                Bucket=backup_bucket, 
                Key=temp_manifest_key, 
                UploadId=upload_id
            )
            return None

        # Completar multipart upload
        result = s3_client.complete_multipart_upload(
            Bucket=backup_bucket,
            Key=temp_manifest_key,
            UploadId=upload_id,
            MultipartUpload={"Parts": parts},
        )

        logger.info(f"‚úÖ Manifiesto completado: {objects_found} objetos en s3://{backup_bucket}/{temp_manifest_key}")
        
        return {
            "bucket": backup_bucket,
            "key": temp_manifest_key,
            "etag": result["ETag"].strip('"'),
        }

    except Exception as e:
        logger.error(f"‚ùå Error durante streaming del inventario: {e}", exc_info=True)
        s3_client.abort_multipart_upload(
            Bucket=backup_bucket, 
            Key=temp_manifest_key, 
            UploadId=upload_id
        )
        raise


# ============================================================================
# FALLBACK: MANIFEST FROM DIRECT LISTING
# ============================================================================

def generate_manifest_from_listing(
    backup_bucket: str,
    source_bucket: str,
    backup_type: str,
    criticality: str,
    last_checkpoint: Optional[datetime],
) -> Optional[Dict[str, str]]:
    """
    Genera un manifiesto CSV recorriendo objetos del bucket origen usando ListObjectsV2.
    Fallback para cuando a√∫n no existe inventario S3.
    """
    
    logger.warning("‚ö†Ô∏è FALLBACK MODE: Generando manifiesto por listado directo")
    
    temp_manifest_key = f"manifests/temp/{source_bucket}-{uuid.uuid4()}.csv"

    mpu = s3_client.create_multipart_upload(
        Bucket=backup_bucket,
        Key=temp_manifest_key,
        ServerSideEncryption='AES256'  # ‚úÖ Cifrado unificado
    )

    upload_id = mpu["UploadId"]
    parts = []
    part_number = 1
    buffer = io.StringIO()
    writer = csv.writer(buffer)
    objects_found = 0

    prefixes = ALLOWED_PREFIXES.get(criticality, [])
    prefixes = prefixes if prefixes else [""]

    try:
        start_time = datetime.now(timezone.utc)
        
        for pfx in prefixes:
            kwargs = {"Bucket": source_bucket}
            if pfx:
                kwargs["Prefix"] = pfx

            paginator = s3_client.get_paginator("list_objects_v2")
            
            for page in paginator.paginate(**kwargs):
                for obj in page.get("Contents", []):
                    key = obj["Key"]
                    last_mod = obj.get("LastModified")
                    
                    # Filtrar por checkpoint
                    if last_checkpoint and last_mod and last_mod <= last_checkpoint:
                        continue

                    writer.writerow([source_bucket, key])
                    objects_found += 1

                    # L√≠mites de seguridad
                    if FALLBACK_MAX_OBJECTS and objects_found >= FALLBACK_MAX_OBJECTS:
                        logger.warning(f"‚ö†Ô∏è Alcanzado l√≠mite de objetos: {FALLBACK_MAX_OBJECTS}")
                        break
                    
                    if FALLBACK_TIME_LIMIT_SECONDS:
                        elapsed = (datetime.now(timezone.utc) - start_time).total_seconds()
                        if elapsed >= FALLBACK_TIME_LIMIT_SECONDS:
                            logger.warning(f"‚ö†Ô∏è Alcanzado l√≠mite de tiempo: {FALLBACK_TIME_LIMIT_SECONDS}s")
                            break

                    # Subir parte cuando se alcanza el tama√±o m√≠nimo
                    if buffer.tell() >= MIN_CHUNK_SIZE_BYTES:
                        part = s3_client.upload_part(
                            Bucket=backup_bucket,
                            Key=temp_manifest_key,
                            PartNumber=part_number,
                            UploadId=upload_id,
                            Body=buffer.getvalue().encode("utf-8"),
                        )
                        parts.append({"PartNumber": part_number, "ETag": part["ETag"]})
                        part_number += 1
                        buffer = io.StringIO()
                        writer = csv.writer(buffer)

        # Subir √∫ltima parte
        if buffer.tell() > 0:
            part = s3_client.upload_part(
                Bucket=backup_bucket,
                Key=temp_manifest_key,
                PartNumber=part_number,
                UploadId=upload_id,
                Body=buffer.getvalue().encode("utf-8"),
            )
            parts.append({"PartNumber": part_number, "ETag": part["ETag"]})

        if objects_found == 0:
            logger.info("‚ÑπÔ∏è Fallback: No se encontraron objetos.")
            s3_client.abort_multipart_upload(
                Bucket=backup_bucket, 
                Key=temp_manifest_key, 
                UploadId=upload_id
            )
            return None

        result = s3_client.complete_multipart_upload(
            Bucket=backup_bucket,
            Key=temp_manifest_key,
            UploadId=upload_id,
            MultipartUpload={"Parts": parts},
        )

        logger.info(f"‚úÖ Fallback completado: {objects_found} objetos en s3://{backup_bucket}/{temp_manifest_key}")
        
        return {
            "bucket": backup_bucket,
            "key": temp_manifest_key,
            "etag": result["ETag"].strip('"')
        }

    except Exception as e:
        logger.error(f"‚ùå Error durante fallback: {e}", exc_info=True)
        s3_client.abort_multipart_upload(
            Bucket=backup_bucket, 
            Key=temp_manifest_key, 
            UploadId=upload_id
        )
        raise


# ============================================================================
# LAMBDA HANDLER
# ============================================================================

def lambda_handler(event: Dict, context) -> Dict:
    """
    Handler principal para filtrar inventario y generar manifiestos.
    """
    try:
        backup_bucket = event["backup_bucket"]
        source_bucket = event["source_bucket"]
        backup_type = event["backup_type"]
        inventory_prefix = event["inventory_key"]
        criticality = event.get("criticality", "MenosCritico")

        logger.info("="*80)
        logger.info(f"üöÄ Iniciando filter_inventory")
        logger.info(f"   Source: {source_bucket}")
        logger.info(f"   Tipo: {backup_type}")
        logger.info(f"   Criticidad: {criticality}")
        logger.info("="*80)

        # Buscar inventario existente
        manifest_key = find_latest_inventory_manifest(backup_bucket, inventory_prefix)
        
        # Leer checkpoint
        last_cp = read_checkpoint(backup_bucket, source_bucket, backup_type)

        # Determinar tipo efectivo de backup
        effective_backup_type = backup_type
        if backup_type == "incremental" and last_cp is None and FORCE_FULL_ON_FIRST_RUN:
            logger.info("üîÑ Primera corrida: forzando FULL backup")
            effective_backup_type = "full"

        # Generar manifiesto
        if manifest_key:
            logger.info("üì¶ Usando inventario S3 existente")
            manifest_obj = s3_client.get_object(Bucket=backup_bucket, Key=manifest_key)
            manifest_data = json.loads(manifest_obj["Body"].read().decode("utf-8"))
            
            manifest_info = stream_inventory_to_manifest(
                backup_bucket, 
                manifest_data, 
                source_bucket, 
                effective_backup_type, 
                criticality, 
                None if effective_backup_type == "full" else last_cp
            )
        else:
            logger.warning("‚ö†Ô∏è No hay inventario disponible. Usando fallback con ListObjectsV2")
            manifest_info = generate_manifest_from_listing(
                backup_bucket, 
                source_bucket, 
                effective_backup_type, 
                criticality, 
                None if effective_backup_type == "full" else last_cp
            )

        if not manifest_info:
            logger.info("‚ÑπÔ∏è No hay objetos nuevos para respaldar.")
            return {
                "status": "EMPTY", 
                "reason": "No new objects",
                "source_bucket": source_bucket
            }

        # Guardar checkpoint al final
        checkpoint_type = effective_backup_type if FORCE_FULL_ON_FIRST_RUN else backup_type
        write_checkpoint(backup_bucket, source_bucket, checkpoint_type, datetime.now(timezone.utc))

        logger.info("="*80)
        logger.info("‚úÖ Proceso completado exitosamente")
        logger.info(f"   Manifiesto: s3://{manifest_info['bucket']}/{manifest_info['key']}")
        logger.info("="*80)

        return {
            "status": "SUCCESS",
            "manifest": manifest_info,
            "manifest_key": manifest_key,
            "source_bucket": source_bucket,
            "backup_type": effective_backup_type,
            "criticality": criticality,
        }

    except Exception as e:
        logger.error("="*80)
        logger.error(f"‚ùå ERROR FATAL en filter_inventory: {e}")
        logger.error("="*80, exc_info=True)
        return {
            "status": "FAILED", 
            "reason": str(e),
            "source_bucket": event.get("source_bucket", "unknown")
        }